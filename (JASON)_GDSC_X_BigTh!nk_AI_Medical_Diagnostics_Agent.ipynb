{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGTEEQ-H8w-e"
      },
      "source": [
        "# Workshop: Building an AI Medical Diagnosis Agent\n",
        "\n",
        "Welcome! In this workshop, we'll build a smart AI Agent that can:\n",
        "\n",
        "1.  **Chat with a user** to understand their symptoms.\n",
        "2.  **Decide** when it has gathered enough information to move to research.\n",
        "3.  **Use external tools** (like Perplexity's API) to narrow down medical conditions.\n",
        "4.  **Analyze the findings** and the conversation.\n",
        "5.  **Generate a structured report.**\n",
        "\n",
        "Think of it like a simplified version of a preliminary medical consultation. But **IMPORTANT DISCLAIMER:** this is for educational purposes ONLY and is NOT a substitute for real medical professionals (yetğŸ˜‰)\n",
        "\n",
        "**Why is this interesting?**\n",
        "\n",
        "* **LLMs as Orchestrators:** We'll see how Large Language Models (LLMs) like Google's Gemini can evolve beyond only generating text and also *control a workflow*, make decisions, and use tools.\n",
        "* **LangChain & LangGraph:** We'll use these powerful libraries designed to make building complex AI applications easier. LangGraph helps create reliable, step-by-step AI processes through state management.\n",
        "* **Real-world Pattern:** Similar patterns to this one (chat -> gather info -> use tools -> synthesize) are common in many professions, and can be leveraged to automate real jobs.\n",
        "\n",
        "**Prerequisites:** Gemini and Perplexity API. We'll explain the AI concepts as we go!\n",
        "\n",
        "**Let's visualize the basic flow at this link:\n",
        "https://gdsc-x-big-think-ai-workshop.vercel.app/**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkOyUc3xpDSM"
      },
      "source": [
        "## Before we start, you need 2 API keys:\n",
        "1. Get free Gemini API key at https://ai.google.dev/gemini-api/docs/api-key\n",
        "2. Get free Perplexity API Key (by using your school email) at https://www.perplexity.ai/referrals/join\n",
        "\n",
        "(You would need to use credit card information to access the Perplexity API once you create an account. You get 5$ of free API credits per month for Perplexity from your student email account and Gemini API is free. So it will not cost anything.)\n",
        "\n",
        "3. Make a copy of this colab notebook, and open your copy. Select the secrets option on left-side handle, and add the GEMINI_API_KEY and PERPLEXITY_API_KEY fields, pasting the API keys in the text fields. This saves the API keys to colab and make them accessible to the colab notebook we'll use for our workshop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mekvUtk7BBtV"
      },
      "source": [
        "## Step 1: Import libraries we'll be using in the workshop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "On5Lt4-0dfy-",
        "outputId": "dad6102d-bcc5-467b-9631-37a4562ba865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.52 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.55)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.3)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.29.4)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.3.33)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (4.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.17 langchain-google-genai-2.1.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "5f423a4f59874f2da8614254cd823714",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.34-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.55)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.8 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.63-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (0.3.33)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (2.11.3)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.3.34-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.63-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.3.34 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.63 ormsgpack-1.9.1 xxhash-3.5.0\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.33)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.55)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.16)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx) (0.14.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (4.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "# ---- Core Python & Utilities ---- #\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time # used for rate-limiting\n",
        "import requests # for making web requests to Perplexity API\n",
        "from functools import wraps # Helper used for building decorators easily\n",
        "from typing import List, Dict, Any # For type hinting\n",
        "from typing_extensions import Annotated, TypedDict # For advanced type for our state\n",
        "from google.colab import userdata # a secure way to access keys in colab we've saved as secrets\n",
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n",
        "\n",
        "# ---- Langchain, LangGraph & Google API ---- #\n",
        "from operator import itemgetter\n",
        "import google.api_core.exceptions # for handling google API errors\n",
        "from google.generativeai import configure, list_models #\n",
        "!pip install langchain-google-genai # installs the LangChain Integration for google's models quietly\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "!pip install langgraph # installs the LangGraph library quietly\n",
        "from langgraph.graph import StateGraph, START, END # Core components for building the graph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "!pip install -U langchain langsmith httpx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnoJP4B4BLCo"
      },
      "source": [
        "## Step 2: Set up the APIs to use LLMs for the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2Dqw2rWBIJ8"
      },
      "outputs": [],
      "source": [
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "PERPLEXITY_API_KEY = userdata.get('PERPLEXITY_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA8FD6E3BkFT"
      },
      "source": [
        "## Extra step: Set up a function for API rate-limiting\n",
        "\n",
        "The functions are delayed for a few seconds every time it runs an API. Without strict controls, repeated or recursive API calls can quickly go out of control, leading to infinite loops or too many API requests in a few seconds. The result ? High billing cost, service denial by API provider, or even temporary bans.\n",
        "\n",
        "We will also be serving Perplexity API as a tool for Gemini LLM to invoke, so there's a chance of infinite loops in case Gemini decides to overdo the research."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN7ctE80BkXz"
      },
      "outputs": [],
      "source": [
        "def api_rate_limit(seconds: int = 2): # default pause is 2 seconds\n",
        "    \"\"\"This nested function creates and returns a Decorator to add sleep time between API calls\"\"\"\n",
        "    def decorator(func):\n",
        "        @wraps(func) # Saves the metadata of wrapped function (like name, docstring) to\n",
        "        def wrapper(*args, **kwargs):\n",
        "            \"\"\"This wrapper executes the following code before the target function executes\"\"\"\n",
        "            time.sleep(seconds)  # Pause for some seconds before making the API call\n",
        "            return func(*args, **kwargs) # Now calls the original target function\n",
        "        return wrapper\n",
        "    return decorator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txL2sCsIDCMg"
      },
      "source": [
        "## Step 3: Defining the perplexity_research function as a tool for LLM to invoke\n",
        "\n",
        "Turning Perplexity API into a tool that Learn-lm-1.5-pro can use to deepen its analysis. The research tool is one of the most crucial steps as it turns a general LLM into an expert on any topic, giving it tools to research the web in real-time and augment its knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eMCkaYNDCe8"
      },
      "outputs": [],
      "source": [
        "@tool # LangChain decorator. Now the function for Perplexity API is available as a tool for the LLM\n",
        "@api_rate_limit(1) # Apply our custom 1-second rate limit before calling this function each time\n",
        "def perplexity_research(query: str) -> str:\n",
        "    \"\"\"Research medical conditions using Perplexity API. Provice citations and links to reliable, authentic research sources.\"\"\"\n",
        "    headers = { # Standard HTTP headers for the API request\n",
        "        \"accept\": \"application/json\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {PERPLEXITY_API_KEY}\"\n",
        "    }\n",
        "    payload = { # The actual data sent to the Perplexity API\n",
        "        \"model\": \"sonar-pro\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\",\n",
        "            \"content\": \"You are a medical research assistant. Provide precise and well-sourced responses, along with citations, and links for resources\"},\n",
        "            {\"role\": \"user\", \"content\": query}\n",
        "        ],\n",
        "        \"temperature\": 0.3,  # Lower randomness for factual consistency\n",
        "        \"max_tokens\": 2048,  # Allow more detailed responses\n",
        "        \"top_p\": 0.8,  # Nucleus sampling for high-confidence outputs\n",
        "        \"frequency_penalty\": 0.0,  # Reduce repetitive phrasing\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(\"RESPONSE: Sending request to Perplexity API...\")\n",
        "        response = requests.post(\"https://api.perplexity.ai/chat/completions\", json=payload, headers=headers)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Debugging API response\n",
        "        json_response = response.json()\n",
        "        print(f\"RESPONSE: API Response JSON: {json_response}\")\n",
        "\n",
        "        # Adjust parsing based on actual response structure\n",
        "        return json_response[\"choices\"][0].get(\"message\", {}).get(\"content\", \"No content found.\")\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"RESPONSE: API Error Details: {str(e)}\")\n",
        "        return f\"Error researching topic: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGFO53x5DSOQ"
      },
      "source": [
        "## Step 4: Set up the LLM for interaction with user input and orchestration with the conversation flow\n",
        "\n",
        "Now that the tool for LLM has been built, let's bind it to the LLM and set up the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91eNyqkWDSi0"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"learnlm-1.5-pro-experimental\",  # You can try other models by replacing with \"gemini-1.5-pro\", \"gemini-1.5-flash\", or \"gemini-2.5-pro-preview-03-25\"\n",
        "    google_api_key=GEMINI_API_KEY,\n",
        "    temperature=0.3 # Range is usually 0 to 1, we are choosing lower value for more predictable responses\n",
        ")\n",
        "# Define a simple prompt template to turn questions into prompt for LLM\n",
        "template = \"Answer this to the best of your knowledge. {question} ?\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "# Bind tools with LLM using LangChain\n",
        "tools = [perplexity_research]\n",
        "llm_with_tools = llm.bind_tools(tools=tools)\n",
        "\n",
        "# State Management\n",
        "# Defines the structure for managing conversation state and analysis progress\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[List[Dict[str, Any]], \"Chat messages\"]                            # Store a list of all chat messages\n",
        "    research_results: Annotated[Dict[str, Any], \"Medical research data\"]                  # Research results from Perplexity API\n",
        "    analysis_complete: Annotated[bool, \"Whether analysis is complete\"]                    # Determine if analysis is completed, in True or False\n",
        "    report: Annotated[Dict[str, Any], \"Final medical analysis report\"]                    # Final report to be returned\n",
        "    conversation_stage: Annotated[str, \"Current stage: conversation, research, complete\"] # Track the current stage for LLM\n",
        "    symptom_details: Annotated[Dict[str, Any], \"Collected symptom information\"]           # Details of symptoms, to be used by LLM\n",
        "    question_count: Annotated[int, \"Number of questions asked so far\"]                    # Not necessary unless we want min/max number of questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XypHmNBfDiXu"
      },
      "source": [
        "## Step 5: Setup the Conversation Flow Nodes to enable deeper research, analysis and report generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEtTVwG5Di-7"
      },
      "outputs": [],
      "source": [
        "# Initial Conversation Handler\n",
        "# Processes user input and generates initial response using STRUCTURED OUTPUT\n",
        "@api_rate_limit(1)\n",
        "def interactive_conversation(state: State):\n",
        "    \"\"\"Handle multi-turn conversation using structured JSON output from LLM\n",
        "       to dynamically decide when enough detail is present.\"\"\"\n",
        "    print(\"PROCESSING: Entering interactive_conversation node...\")\n",
        "    current_messages = state[\"messages\"]\n",
        "    question_count = state.get(\"question_count\", 0) + 1 # Still track for context/failsafe\n",
        "    symptom_details = state.get(\"symptom_details\", {})\n",
        "\n",
        "    # --- Failsafe Check (Optional but Recommended) ---\n",
        "    FAILSAFE_LIMIT = 10 # Set a max limit of questions to prevent potential infinite loops\n",
        "    if question_count > FAILSAFE_LIMIT:\n",
        "        print(f\"DEBUG ERROR: Failsafe question limit ({FAILSAFE_LIMIT}) reached. Forcing move to research.\")\n",
        "        # Update symptom details one last time\n",
        "        if current_messages and current_messages[-1].get(\"role\") == \"user\":\n",
        "            last_updated = symptom_details.get(\"last_updated\", -1)\n",
        "            if len(current_messages) > last_updated:\n",
        "                symptom_details = extract_symptom_details(current_messages)\n",
        "\n",
        "        # Construct a hardcoded message indicating the move to analysis due to limit.\n",
        "        response_content = \"Based on the information gathered so far, I will now proceed with the analysis.\"\n",
        "        new_message = {\"role\": \"assistant\", \"content\": response_content}\n",
        "        updated_messages = current_messages + [new_message]\n",
        "        return {\n",
        "            \"messages\": updated_messages,\n",
        "            \"question_count\": question_count -1, # Stay at the failsafe count\n",
        "            \"conversation_stage\": \"research\", # Force stage to research\n",
        "            \"symptom_details\": symptom_details\n",
        "        }\n",
        "    # --- End Failsafe Check ---\n",
        "\n",
        "    # Update symptom details if new user message arrived\n",
        "    if current_messages and current_messages[-1].get(\"role\") == \"user\":\n",
        "        last_updated = symptom_details.get(\"last_updated\", -1)\n",
        "        if len(current_messages) > last_updated:\n",
        "            print(\"PROCESSING: (interactive_conversation) Extracting details from latest user message...\")\n",
        "            symptom_details = extract_symptom_details(current_messages)\n",
        "\n",
        "\n",
        "    # --- Prompt requesting JSON output ---\n",
        "    prompt = f\"\"\"\n",
        "    {SYSTEM_PROMPT}\n",
        "\n",
        "    You are in the **information gathering** stage of a medical consultation. Your goal is to gather sufficient detail to perform a preliminary analysis following a standard procedure.\n",
        "    Conversation History:\n",
        "    {format_conversation_history(current_messages)} # Assuming this helper exists\n",
        "\n",
        "    Current Symptom Understanding (internal summary - may be incomplete):\n",
        "    {symptom_details.get(\"extracted_data\", \"No structured summary yet.\")}\n",
        "\n",
        "    Based on the conversation history and your understanding:\n",
        "\n",
        "    1.  **Assess Sufficiency:** Do you have enough detail about the main complaints? Consider key aspects like the following, but remember ALL of these are not always\n",
        "        useful. Based on what you think the conditions could be, you decide only the RELEVANT pieces of info to ask:\n",
        "Â  Â  Â  Â  * Onset & Duration\n",
        "Â  Â  Â  Â  * Location & Radiation\n",
        "Â  Â  Â  Â  * Quality/Character (e.g., sharp, dull, pressure)\n",
        "Â  Â  Â  Â  * Severity (e.g., scale of 1-10 if appropriate, or description)\n",
        "Â  Â  Â  Â  * Timing/Frequency\n",
        "Â  Â  Â  Â  * Aggravating/Alleviating Factors\n",
        "Â  Â  Â  Â  * Associated Symptoms\n",
        "Â  Â  Â  Â  * Relevant Medical History (briefly, if mentioned)\n",
        "\n",
        "    2.  **Decide Action and Format Output:** Respond ONLY with a valid JSON object containing two keys:\n",
        "        * `\"proceed_to_research\"`: A boolean value (`True` if you have sufficient detail based on the criteria, `False` otherwise).\n",
        "        * `\"assistant_message\"`: (string)\n",
        "   Â  Â  Â  Â  Â  * If `True`, a brief, empathetic confirmation (e.g., \"Thank you for sharing that detail. I think I have enough information to proceed with the next step.\").\n",
        "   Â  Â  Â  Â  Â  * If `False`, the single, most important follow-up question needed right now. Keep it concise and empathetic (e.g., \"Could you tell me more about when this symptom started?\").\n",
        "\n",
        "    **CRITICAL INSTRUCTION:** Even if you assess the situation as potentially requiring immediate emergency care, **do not** include that assessment or recommendation in the `assistant_message` *at this stage*. Stick strictly to the JSON format and the content rules described above. Emergency considerations are handled later.\n",
        "\n",
        "    Example valid JSON output if continuing conversation:\n",
        "    {{\n",
        "      \"proceed_to_research\": False,\n",
        "      \"assistant_message\": \"When you feel short of breath, does anything seem to make it better or worse?\"\n",
        "    }}\n",
        "\n",
        "    Example valid JSON output if ready for research:\n",
        "    {{\n",
        "      \"proceed_to_research\": True,\n",
        "      \"assistant_message\": \"Thank you. I have enough information to analyze your symptoms now.\"\n",
        "    }}\n",
        "\n",
        "    This is conversation turn {question_count}. Ensure your entire response is ONLY the JSON object without any introductory text or explanation.\n",
        "    \"\"\"\n",
        "    print(f\"DEBUG: Invoking LLM for conversation (Turn {question_count}, assessing sufficiency, expecting JSON)...\")\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        response_content = response.content if hasattr(response, 'content') else str(response)\n",
        "        print(f\"DEBUG: LLM raw response received: '{response_content[:100]}...'\") # Log more for debugging JSON\n",
        "\n",
        "        # --- Attempt to Parse JSON Response ---\n",
        "        try:\n",
        "            # Clean potential markdown code fences if the model wraps JSON in them\n",
        "            if response_content.strip().startswith(\"```json\"):\n",
        "                response_content = response_content.strip()[7:-3].strip()\n",
        "            elif response_content.strip().startswith(\"```\"):\n",
        "                 response_content = response_content.strip()[3:-3].strip()\n",
        "\n",
        "            parsed_data = json.loads(response_content)\n",
        "\n",
        "            # Validate expected keys and types (basic validation)\n",
        "            if not isinstance(parsed_data, dict) or \\\n",
        "               \"proceed_to_research\" not in parsed_data or \\\n",
        "               \"assistant_message\" not in parsed_data or \\\n",
        "               not isinstance(parsed_data[\"proceed_to_research\"], bool) or \\\n",
        "               not isinstance(parsed_data[\"assistant_message\"], str):\n",
        "                raise ValueError(\"Parsed JSON missing required keys or has incorrect types.\")\n",
        "\n",
        "            has_enough_info = parsed_data[\"proceed_to_research\"]\n",
        "            assistant_content = parsed_data[\"assistant_message\"]\n",
        "            print(f\"DEBUG: JSON parsed successfully. proceed_to_research={has_enough_info}\")\n",
        "\n",
        "        except (json.JSONDecodeError, ValueError) as json_error:\n",
        "            print(f\"ERROR: Failed to parse valid JSON or validate structure from LLM response: {json_error}\")\n",
        "            print(f\"LLM Raw Response causing error: {response_content}\")\n",
        "            has_enough_info = False # Default to continuing conversation on format error\n",
        "            assistant_content = \"I seem to be having trouble formatting my thoughts. Could you please clarify your last point or ask again?\"\n",
        "            # Optionally, you could use the raw response_content here if it might be readable\n",
        "\n",
        "    except Exception as llm_error:\n",
        "        print(f\"ERROR: LLM invocation failed in interactive_conversation: {llm_error}\")\n",
        "        has_enough_info = False # Default to continuing\n",
        "        assistant_content = \"I encountered an issue communicating. Could you please try again?\"\n",
        "        # No new_stage variable needed here as it's determined after the try-except block\n",
        "\n",
        "    # Determine the next stage based on the parsed boolean flag\n",
        "    new_stage = \"research\" if has_enough_info else \"conversation\"\n",
        "    print(f\"DEBUG: Based on parsed JSON/error handling: enough info? {has_enough_info}. New stage: {new_stage}\")\n",
        "\n",
        "    # Use the extracted message content\n",
        "    new_message = {\"role\": \"assistant\", \"content\": assistant_content}\n",
        "    updated_messages = current_messages + [new_message]\n",
        "\n",
        "    return {\n",
        "        \"messages\": updated_messages,\n",
        "        \"question_count\": question_count,\n",
        "        \"conversation_stage\": new_stage,\n",
        "        \"symptom_details\": symptom_details\n",
        "    }\n",
        "    # --- End Pre-check ---\n",
        "\n",
        "    # Update symptom details if new user message arrived since last extraction\n",
        "    if current_messages and current_messages[-1].get(\"role\") == \"user\":\n",
        "        last_updated = symptom_details.get(\"last_updated\", -1)\n",
        "        if len(current_messages) > last_updated:\n",
        "            symptom_details = extract_symptom_details(current_messages)\n",
        "\n",
        "    # Define prompt for the information gathering stage\n",
        "    prompt = f\"\"\"\n",
        "    {SYSTEM_PROMPT}\n",
        "\n",
        "    You are in the information gathering stage. This is question number {question_count}. Here's the conversation so far:\n",
        "    {format_conversation_history(current_messages)}\n",
        "\n",
        "    Based on this information, ask **ONE** specific, relevant follow-up question to gather more details about the symptoms already mentioned (like duration, progression, aggravating/alleviating factors, associated symptoms, relevant history).\n",
        "\n",
        "    Alternatively, if you assess that you have sufficient detail about the main symptoms (e.g., at least 3-4 different symptoms or aspects clarified), respond ONLY with the exact phrase: \"I have enough information to analyze your symptoms now.\"\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"DEBUG: Invoking LLM for conversation (Question {question_count})...\")\n",
        "    response = llm.invoke(prompt)\n",
        "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
        "    print(f\"DEBUG: LLM response received: '{response_content[:100]}...'\")\n",
        "\n",
        "    # Check if the LLM decided it has enough information\n",
        "    has_enough_info = \"enough information\" in response_content.lower()\n",
        "\n",
        "    # Determine the next stage based ONLY on LLM response now (count check is done above)\n",
        "    new_stage = \"research\" if has_enough_info else \"conversation\"\n",
        "    print(f\"DEBUG: LLM indicated enough info? {has_enough_info}. New stage: {new_stage}\")\n",
        "\n",
        "    new_message = {\"role\": \"assistant\", \"content\": response_content}\n",
        "    updated_messages = current_messages + [new_message]\n",
        "\n",
        "    return {\n",
        "        \"messages\": updated_messages,\n",
        "        \"question_count\": question_count, # Pass the current count along\n",
        "        \"conversation_stage\": new_stage,\n",
        "        \"symptom_details\": symptom_details\n",
        "    }\n",
        "\n",
        "def format_conversation_history(messages):\n",
        "    \"\"\"Format the conversation history for the LLM prompt\"\"\"\n",
        "    formatted = \"\"\n",
        "    for msg in messages:\n",
        "        # Ensure content exists and is a string\n",
        "        content = msg.get(\"content\", \"\")\n",
        "        if not isinstance(content, str):\n",
        "             content = str(content) # Convert non-strings\n",
        "\n",
        "        role = \"User\" if msg.get(\"role\") == \"user\" else \"Assistant\"\n",
        "        formatted += f\"{role}: {content}\\n\\n\"\n",
        "    return formatted.strip() # Remove trailing newline\n",
        "\n",
        "@api_rate_limit(1) # Add rate limiting if desired\n",
        "def extract_symptom_details(messages):\n",
        "    \"\"\"Extract symptom information from user messages using LLM\"\"\"\n",
        "    # Combine relevant user messages\n",
        "    user_input_list = [\n",
        "        str(msg.get(\"content\", \"\")) # Ensure content is string\n",
        "        for msg in messages\n",
        "        if msg.get(\"role\") == \"user\"\n",
        "    ]\n",
        "    if not user_input_list:\n",
        "         return {\"extracted_data\": \"No user input found\", \"last_updated\": len(messages)}\n",
        "\n",
        "    all_user_input = \"\\n\".join(user_input_list)\n",
        "\n",
        "    extract_prompt = f\"\"\"\n",
        "    Based on the following user messages, extract and structure key symptom information:\n",
        "\n",
        "    {all_user_input}\n",
        "\n",
        "    Organize details into: Primary symptoms (list with severity/duration if mentioned), Associated symptoms, Timing/Patterns, Aggravating/Relieving factors, Relevant medical history.\n",
        "    Return as concise, structured text (not strict JSON).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"DEBUG: Extracting symptom details...\")\n",
        "        response = llm.invoke(extract_prompt)\n",
        "        extracted_content = response.content if hasattr(response, 'content') else str(response)\n",
        "        print(\"DEBUG: Symptom extraction complete.\")\n",
        "        return {\"extracted_data\": extracted_content, \"last_updated\": len(messages)}\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting symptom details: {str(e)}\")\n",
        "        # Provide error information but allow flow to continue\n",
        "        return {\"extracted_data\": f\"Error processing symptoms: {str(e)}\", \"last_updated\": len(messages)}\n",
        "\n",
        "# --- Placeholder for Waiting ---\n",
        "# This node doesn't do anything, it's just a named step in the graph\n",
        "# where the control flow pauses before the next user input in the command line loop.\n",
        "def wait_for_user_response(state: State):\n",
        "     \"\"\"Node indicating the graph is waiting for user input.\"\"\"\n",
        "     print(\"DEBUG: Entering wait_for_user_response node (waiting for input loop)...\")\n",
        "     # No state change needed here, just a logical pause point\n",
        "     return state\n",
        "\n",
        "# Research Determination\n",
        "# Analyzes symptoms and queries medical research\n",
        "# Modify the determine_research_needs function to explicitly use the tool:\n",
        "# --- Research Node ---\n",
        "# No rate-limit needed here as it calls perplexity_research, which already has rate-limiting\n",
        "def determine_research_needs(state: State):\n",
        "    \"\"\"Determine what conditions to research based on conversation.\"\"\"\n",
        "    print(\"DEBUG: Entering determine_research_needs node...\")\n",
        "    messages = state[\"messages\"]\n",
        "    symptom_details = state.get(\"symptom_details\", {})\n",
        "\n",
        "    # Use the structured details if available, otherwise fall back to user messages\n",
        "    extracted_data = symptom_details.get(\"extracted_data\", \"No structured data extracted.\")\n",
        "    if extracted_data == \"No structured data extracted.\" or \"Error processing symptoms\" in extracted_data:\n",
        "         # Fallback: use raw user input if extraction failed or didn't happen\n",
        "         user_input_list = [str(msg.get(\"content\",\"\")) for msg in messages if msg.get(\"role\") == \"user\"]\n",
        "         symptom_summary_for_research = \"\\n\".join(user_input_list)\n",
        "         print(\"DEBUG: Using raw user input for research prompt as structured data is unavailable/error.\")\n",
        "    else:\n",
        "        symptom_summary_for_research = extracted_data\n",
        "        print(\"DEBUG: Using extracted symptom details for research prompt.\")\n",
        "\n",
        "\n",
        "    research_prompt = f\"\"\"\n",
        "    Based on the following symptom information:\n",
        "    {symptom_summary_for_research}\n",
        "\n",
        "    Perform medical research focusing on:\n",
        "    1. Most probable conditions (ranked).\n",
        "    2. Brief explanation, causes, risk factors for each.\n",
        "    3. Cite relevant, authoritative sources (e.g., Mayo Clinic, NIH, PubMed links if possible).\n",
        "    4. Suggest potential diagnostic steps.\n",
        "    \"\"\"\n",
        "    print(\"RESPONSE: Starting Perplexity research...\")\n",
        "    # Ensure the tool gets a dictionary with 'query' key\n",
        "    results = perplexity_research.invoke({\"query\": research_prompt})\n",
        "    print(\"RESPONSE: Perplexity research complete.\")\n",
        "\n",
        "    # Store results correctly\n",
        "    return {\"research_results\": {\"medical_research\": results}} # Ensure results are nested if needed later\n",
        "\n",
        "# Processes research data and generates medical analysis\n",
        "# --- Analysis Node ---\n",
        "@api_rate_limit(1)\n",
        "def generate_analysis(state: State):\n",
        "    \"\"\"Generate medical analysis incorporating research.\"\"\"\n",
        "    print(\"DEBUG: Entering generate_analysis node...\")\n",
        "    # Correctly access nested research results\n",
        "    research_data = state.get('research_results', {}).get('medical_research', 'No research data available.')\n",
        "    messages = state[\"messages\"]\n",
        "    symptom_details = state.get(\"symptom_details\", {})\n",
        "\n",
        "    # Prepare symptom summary for analysis prompt\n",
        "    extracted_data = symptom_details.get(\"extracted_data\", \"No structured data.\")\n",
        "    if extracted_data == \"No structured data.\" or \"Error processing symptoms\" in extracted_data:\n",
        "         user_input_list = [str(msg.get(\"content\",\"\")) for msg in messages if msg.get(\"role\") == \"user\"]\n",
        "         symptom_summary_for_analysis = \"\\n\".join(user_input_list)\n",
        "    else:\n",
        "        symptom_summary_for_analysis = extracted_data\n",
        "\n",
        "    analysis_prompt = f\"\"\"\n",
        "    {SYSTEM_PROMPT}\n",
        "    Generate a detailed medical analysis based on the conversation and research.\n",
        "    Format the entire report using Markdown syntax. Use headings (e.g., `## Section Title` or `**Section Title:**`), bullet points (`* point` or `- point`), and\n",
        "    bold text (`**important**`) for clarity and readability.\n",
        "    IMPORTANT: Ensure your entire report uses standard UTF-8 encoding. Avoid generating non-printable control characters. Use only widely compatible Markdown syntax (headings, lists, bold, italics, standard tables).\n",
        "\n",
        "    SYMPTOM SUMMARY:\n",
        "    {symptom_summary_for_analysis}\n",
        "\n",
        "    RESEARCH FINDINGS:\n",
        "    {research_data}\n",
        "\n",
        "    Your analysis report should include:\n",
        "    1. Summary of key symptoms and risk factors (from conversation).\n",
        "    2. Differential diagnosis: Ranked list of probable conditions with confidence scores (e.g., Use percentages strongly supported by research for specific criteria. Justify ranking briefly.\n",
        "    3. Explanation of top 2-3 conditions (causes, symptoms matching/not matching).\n",
        "    4. Recommended next steps (e.g., see primary care, specialist, diagnostics mentioned in research).\n",
        "    5. **Crucially:** Reiterate if any symptoms warrant **immediate emergency care**. Include standard medical disclaimers.\n",
        "    \"\"\"\n",
        "    print(\"DEBUG: Invoking LLM for analysis generation...\")\n",
        "    analysis_response = llm.invoke(analysis_prompt)\n",
        "    analysis_content = analysis_response.content if hasattr(analysis_response, 'content') else str(analysis_response)\n",
        "    print(\"DEBUG: Analysis generation complete.\")\n",
        "    return {\"analysis_complete\": True, \"report\": {\"content\": analysis_content}} # Store content correctly\n",
        "\n",
        "# ----Final Response Formation----\n",
        "def final_response(state: State):\n",
        "    \"\"\"Format the final report for the user.\"\"\"\n",
        "    print(\"DEBUG: Entering final_response node...\")\n",
        "    report_content = state.get(\"report\", {}).get(\"content\", \"Analysis could not be generated.\")\n",
        "    final_message = {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": f\"--- Medical Analysis Report ---\\n\\n{report_content}\\n\\n--- End of Report ---\"\n",
        "    }\n",
        "    print(\"DEBUG: Final response formatted.\")\n",
        "\n",
        "    # Add a final message and ensure stage is 'complete'\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [final_message],\n",
        "        \"conversation_stage\": \"complete\",\n",
        "        \"analysis_complete\": True, # Ensure this is set to finish the loop\n",
        "        \"report\": state[\"report\"] # Pass report through\n",
        "    }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XcfLpnagjN8"
      },
      "source": [
        "# Step 6: Flow Control Functions\n",
        "\n",
        "These functions help provide conditional flow to the Diagnostics Agent, helping it make decisions on whether to to research or not, if analysis is completed, and when to reset conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yujR8yk2f903"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Research Decision Logic\n",
        "# Determines if additional research is needed\n",
        "def should_research(state: State) -> str:\n",
        "    \"\"\"Determine if research is needed based on message content\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1][\"content\"]\n",
        "\n",
        "    # Always do research for medical queries for now, can be modified for more complex applications\n",
        "    if any(term in last_message.lower() for term in [\"symptoms\", \"pain\", \"feeling\", \"medical\", \"health\"]):\n",
        "        return \"research\"\n",
        "    return \"generate_analysis\"\n",
        "\n",
        "# Analysis Completion Check to verify if the medical analysis is complete\n",
        "def is_analysis_complete(state: State) -> str:\n",
        "    \"\"\"Check if analysis is complete or if further conversation is needed.\"\"\"\n",
        "    # Simplified logic without LLM call\n",
        "    return \"complete\" if state.get(\"analysis_complete\") else \"intake_conversation\"\n",
        "\n",
        "# --- Reset Conversation Node ---\n",
        "def reset_conversation(state: State):\n",
        "    \"\"\"Reset the state for a new topic, keeping only the last user message.\"\"\"\n",
        "    print(\"RESTART: Entering reset_conversation node...\")\n",
        "    last_user_message = None\n",
        "    if state[\"messages\"] and state[\"messages\"][-1].get(\"role\") == \"user\":\n",
        "         last_user_message = state[\"messages\"][-1]\n",
        "\n",
        "    # Acknowledge the reset\n",
        "    acknowledgment = {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Okay, let's focus on this new topic. Please tell me about the new symptoms or concerns you have.\"\n",
        "    }\n",
        "\n",
        "    # Start new history\n",
        "    new_messages = [last_user_message, acknowledgment] if last_user_message else [acknowledgment]\n",
        "\n",
        "    # Return a fully reset state dictionary\n",
        "    return {\n",
        "        \"messages\": new_messages,\n",
        "        \"research_results\": {},\n",
        "        \"analysis_complete\": False,\n",
        "        \"report\": {},\n",
        "        \"conversation_stage\": \"conversation\", # Back to starting conversation stage\n",
        "        \"symptom_details\": {},\n",
        "        \"question_count\": 0\n",
        "    }\n",
        "\n",
        "def determine_next_stage(state: State) -> str:\n",
        "    \"\"\"Determine the next node or END the current invocation to wait for user.\"\"\"\n",
        "    print(f\"THINKING: Determining next stage... Current stage: {state.get('conversation_stage')}\")\n",
        "    messages = state[\"messages\"]\n",
        "    current_stage = state.get(\"conversation_stage\", \"conversation\")\n",
        "    last_message_role = messages[-1].get(\"role\") if messages else None\n",
        "\n",
        "    # Check if analysis is complete (triggered after final_response runs)\n",
        "    if current_stage == \"complete\":\n",
        "         if last_message_role == \"user\":\n",
        "             last_user_message_content = str(messages[-1].get(\"content\", \"\")).lower()\n",
        "             if any(phrase in last_user_message_content for phrase in [\"new symptom\", \"different issue\", \"another problem\", \"new topic\"]):\n",
        "                 print(\"RESTART: Routing to restart_conversation.\")\n",
        "                 return \"restart_conversation\"\n",
        "             else:\n",
        "                 print(\"END: Routing to END graph (conversation complete, no new topic).\")\n",
        "                 return END # END the graph's execution completely\n",
        "         else: # Last message was assistant's final report\n",
        "             print(\"END: Routing to END graph (final report sent).\")\n",
        "             return END # END the graph's execution completely\n",
        "\n",
        "    # If interactive_conversation decided we need to research\n",
        "    if current_stage == \"research\":\n",
        "        print(\"PROCESSING: Routing to start_research.\")\n",
        "        return \"start_research\"\n",
        "\n",
        "    # If we are in the conversation stage\n",
        "    if current_stage == \"conversation\":\n",
        "        if last_message_role == \"assistant\":\n",
        "            # Assistant just spoke. If it asked a question (didn't say \"enough info\"),\n",
        "            # stop the graph execution here to wait for user input in the external loop.\n",
        "            if \"enough information\" not in str(messages[-1].get(\"content\", \"\")).lower():\n",
        "                 print(\"PROCESSING: Routing to END (yielding for user input).\")\n",
        "                 return END # <<<--- Stops the current invoke call\n",
        "            else:\n",
        "                 # Assistant said \"enough info\", but stage is still 'conversation'.\n",
        "                 # This means interactive_conversation should have set stage to 'research'.\n",
        "                 # The next invoke call will handle the 'research' stage correctly.\n",
        "                 # So, we END the current invoke here.\n",
        "                 print(\"DEBUG: Routing to END (yielding before research stage starts on next invoke).\")\n",
        "                 return END # <<<--- Stops the current invoke call\n",
        "\n",
        "        elif last_message_role == \"user\":\n",
        "            # User just responded, continue the conversation internally\n",
        "            print(\"DEBUG: Routing to continue_conversation.\")\n",
        "            return \"continue_conversation\" # Go back to interactive_conversation node\n",
        "        else: # Initial state\n",
        "            print(\"DEBUG: Routing to continue_conversation (initial state).\")\n",
        "            return \"continue_conversation\"\n",
        "\n",
        "    # Fallback case - should ideally not be reached with proper state management\n",
        "    print(\"ERROR: determine_next_stage fell through. Routing to END.\")\n",
        "    return END # <<<--- Stops the current invoke call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gveUqfviD0pb"
      },
      "source": [
        "## Step 7: Putting it all together with Graph Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E6BbtmyD34D",
        "outputId": "61f9215a-61ed-47ab-ded9-0fa1ad90aab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compiling the revised graph...\n",
            "Revised graph compiled.\n",
            "Could not draw graph: Install pygraphviz to draw graphs: `pip install pygraphviz`.\n"
          ]
        }
      ],
      "source": [
        "# === Build the Multi-Turn Graph ===\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "graph_builder.add_node(\"interactive_conversation\", interactive_conversation)\n",
        "graph_builder.add_node(\"determine_research_needs\", determine_research_needs)\n",
        "graph_builder.add_node(\"generate_analysis\", generate_analysis)\n",
        "graph_builder.add_node(\"final_response\", final_response)\n",
        "graph_builder.add_node(\"reset_conversation\", reset_conversation)\n",
        "\n",
        "# Starting edge\n",
        "graph_builder.add_edge(START, \"interactive_conversation\")\n",
        "\n",
        "# Edges from interactive_conversation based on determine_next_stage\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"interactive_conversation\",\n",
        "    determine_next_stage,\n",
        "    {\n",
        "        \"continue_conversation\": \"interactive_conversation\", # Loop back if user responded\n",
        "        \"start_research\": \"determine_research_needs\",       # Move to research when ready\n",
        "        END: END                                            # Route to graph's END when yielding for user\n",
        "    }\n",
        ")\n",
        "\n",
        "# REMOVED Edges related to wait_for_user\n",
        "\n",
        "# Connect research and analysis flow (remains the same)\n",
        "graph_builder.add_edge(\"determine_research_needs\", \"generate_analysis\")\n",
        "graph_builder.add_edge(\"generate_analysis\", \"final_response\")\n",
        "\n",
        "# End after final response (or handle reset/follow-up from there)\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"final_response\",\n",
        "    determine_next_stage, # Reuse determine stage after final report is added\n",
        "    {\n",
        "        END: END, # Use END directly to terminate graph execution\n",
        "        \"restart_conversation\": \"reset_conversation\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Connect reset node back to conversation (remains the same)\n",
        "graph_builder.add_edge(\"reset_conversation\", \"interactive_conversation\")\n",
        "\n",
        "# Compile the graph\n",
        "print(\"Compiling the revised graph...\")\n",
        "graph = graph_builder.compile()\n",
        "print(\"Revised graph compiled.\")\n",
        "\n",
        "#Optional: Draw the graph again if you like\n",
        "from IPython.display import Image\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_png()))\n",
        "except Exception as e:\n",
        "    print(f\"Could not draw graph: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM1kQEygECOz"
      },
      "outputs": [],
      "source": [
        "# System Prompt defines AI's role and responsibilities in medical analysis\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an advanced AI medical assistant simulating a preliminary diagnostic consultation with access to up-to-date medical literature, expert guidelines, and peer-reviewed studies. Your role is to:\n",
        "1. Conduct a structured diagnostic evaluation, mimicking a board-certified physicianâ€™s approach.\n",
        "2. Use differential diagnosis methods, listing probable conditions with confidence scores assessing the likelihood of each condition.\n",
        "3. Prioritize high-accuracy, medically reviewed sources (such as but not limited to PubMed, Mayo Clinic, NIH, UpToDate).\n",
        "4. Clearly communicate **when emergency medical care might be required**.\n",
        "5. Provide a clear, structured medical report summarizing likely conditions with citations to justify the evaluation, risk assessments, and next steps.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksiO4snWEKab"
      },
      "outputs": [],
      "source": [
        "# Execution Function\n",
        "# Main function to run the medical analysis workflow\n",
        "def run_medical_analysis(initial_message: str):\n",
        "    \"\"\"Runs the medical analysis graph with the given initial message.\"\"\"\n",
        "    initial_state = {\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": initial_message}],\n",
        "        \"research_results\": {},\n",
        "        \"analysis_complete\": False,\n",
        "        \"report\": {}\n",
        "    }\n",
        "\n",
        "    results = graph.invoke(initial_state)\n",
        "    return results[\"messages\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsgwE1H8gMKC"
      },
      "outputs": [],
      "source": [
        "# === Interactive Command Line Execution (with Rich Rendering for better visual output) ===\n",
        "def run_command_line():\n",
        "    \"\"\"Run an interactive demo of the medical chatbot in the command line.\"\"\"\n",
        "    print(\"\\n--- Medical Symptom Analysis Chatbot ---\")\n",
        "    print(\"Describe your symptoms to start.\")\n",
        "    print(\"Type 'exit' to end.\")\n",
        "    print(\"Type 'new topic' (or similar) after analysis to discuss something else.\\n\")\n",
        "\n",
        "    # Instantiate Console *outside* the loop\n",
        "    console = Console()\n",
        "    state = None # initialize state as None\n",
        "\n",
        "    while True:\n",
        "        if not state:\n",
        "            # Start of a new conversation\n",
        "            user_input = input(\"You: \")\n",
        "            if user_input.lower() == 'exit':\n",
        "                break\n",
        "            initial_state_dict = {\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": user_input}],\n",
        "                \"research_results\": {}, \"analysis_complete\": False, \"report\": {},\n",
        "                \"conversation_stage\": \"conversation\", \"symptom_details\": {}, \"question_count\": 0\n",
        "            }\n",
        "            # Invoke the graph to get the first assistant response\n",
        "            try:\n",
        "                print(\"START: Invoking graph (initial)...\")\n",
        "                state = graph.invoke(initial_state_dict, {\"recursion_limit\": 15})\n",
        "                print(\"UPDATE: Graph invocation complete (initial).\")\n",
        "            except Exception as e:\n",
        "                # Use console.print for error messages too, for consistency\n",
        "                console.print(f\"\\n[bold red]ERROR:[/bold red] Graph failed during initial invocation: {e}\")\n",
        "                console.print(\"Please try again or type 'exit'.\")\n",
        "                state = None\n",
        "                continue\n",
        "\n",
        "        else:\n",
        "            # Continue existing conversation\n",
        "            user_input = input(\"You: \")\n",
        "            if user_input.lower() == 'exit':\n",
        "                break\n",
        "\n",
        "            current_messages = state.get(\"messages\", [])\n",
        "            updated_messages = current_messages + [{\"role\": \"user\", \"content\": user_input}]\n",
        "            state[\"messages\"] = updated_messages\n",
        "\n",
        "            try:\n",
        "                print(\"PROCESSING: Invoking graph (continue)...\")\n",
        "                state = graph.invoke(state, {\"recursion_limit\": 15})\n",
        "                print(\"UPDATE: Graph invocation complete (continue).\")\n",
        "            except Exception as e:\n",
        "                console.print(f\"\\n[bold red]ERROR:[/bold red] Graph failed during continuation: {e}\")\n",
        "                if state and state.get(\"messages\"):\n",
        "                     # Try to render the last assistant message before the error, if possible\n",
        "                     last_assistant_message = state[\"messages\"][-1]\n",
        "                     if last_assistant_message.get(\"role\") == \"assistant\":\n",
        "                         console.print(f\"\\n[bold deep_sky_blue1][Assistant]:[/bold deep_sky_blue1]\")\n",
        "                         console.print(Markdown(last_assistant_message.get('content', '[No Content]')))\n",
        "                     else: # Fallback if last message wasn't assistant\n",
        "                         console.print(\"\\n[bold red]Assistant:[/bold red] Sorry, an error occurred.\")\n",
        "                else:\n",
        "                    console.print(\"\\n[bold red]Assistant:[/bold red] Sorry, an error occurred and I lost track of the conversation. Please start over or type 'exit'.\")\n",
        "                    state = None\n",
        "                continue\n",
        "\n",
        "\n",
        "        # --- Process graph output ---\n",
        "        if not state or not state.get(\"messages\"):\n",
        "            console.print(\"\\n[bold red]Assistant:[/bold red] Sorry, something went wrong, and I don't have a response.\")\n",
        "            state = None\n",
        "            continue\n",
        "\n",
        "        # Display the latest assistant message using Rich\n",
        "        assistant_message = state[\"messages\"][-1]\n",
        "        if assistant_message.get(\"role\") == \"assistant\":\n",
        "            # *** Use Rich Console and Markdown Here ***\n",
        "            console.print(f\"\\n[bold deep_sky_blue1][Assistant]:[/bold deep_sky_blue1]\")\n",
        "            markdown_content = Markdown(assistant_message.get('content', '[No Content]'))\n",
        "            console.print(markdown_content)\n",
        "            print() # Add an extra newline for spacing after the rendered block\n",
        "        else:\n",
        "            # Should not happen if graph works correctly\n",
        "            print(\"DEBUG: Expected assistant message, but last message was:\", assistant_message.get(\"role\"))\n",
        "\n",
        "\n",
        "        # Check if the conversation has reached a final state\n",
        "        current_stage = state.get(\"conversation_stage\")\n",
        "        if current_stage == \"complete\":\n",
        "            if \"--- End of Report ---\" in str(assistant_message.get(\"content\", \"\")):\n",
        "                # Use console.print for consistency\n",
        "                console.print(\"\\n[bold green]--- Analysis Complete ---[/bold green]\")\n",
        "                console.print(\"You can ask follow-up questions about this report, type 'new topic' to discuss something else, or type 'exit'.\")\n",
        "            else:\n",
        "                console.print(\"\\n[bold yellow]--- Conversation Ended ---[/bold yellow]\")\n",
        "                break # Exit loop\n",
        "\n",
        "\n",
        "    console.print(\"\\nChat ended.\")\n",
        "\n",
        "def start_interactive_chat():\n",
        "    try:\n",
        "        if not GEMINI_API_KEY or not PERPLEXITY_API_KEY:\n",
        "             print(\"ERROR: API Keys not found. Please set them up in Colab secrets.\")\n",
        "             return\n",
        "        run_command_line() # This now uses the rich-enabled version\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aB3oQE-FpFr"
      },
      "source": [
        "We have the components and the graph blueprint. Now let's run it! We want to test the model and evaluate its quality against a real medical professional, so we will use AI-generated patient data to generate the output, and use AI to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM21ajyAELrD"
      },
      "outputs": [],
      "source": [
        "# Go to chatgpt.com and enter this prompt to test the model against actual input\n",
        "chatgpt_prompt = \"\"\"\n",
        "Create a sample user info for testing an AI Medical Diagnostics Agent.\n",
        "The aim of the agent is to act as an advanced AI medical assistant simulating a preliminary diagnostic consultation\n",
        "with access to up-to-date medical literature, expert guidelines, and peer-reviewed studies. You must generate some input simulating a\n",
        "patient with a certain medical condition. Your input must not make the diagnosis very easy or make it obvious what condition the person is suffering from,\n",
        "because the goal is to test the accuracy and quality of the AI agent as compared against a real medical professional with years of experience.\n",
        "Generate example input for me, dividing it up into sections of text to test the AI agent. Also tell me what the expected diagnosis should be, but\n",
        "make it hard for the agent to figure it out. I may also ask you to answer questions matching that user conversation which were asked from agent,\n",
        "and you must give me the response to share. Keep it short and concise, only share text within 50 words or less per conversation turn.\n",
        "\n",
        "Later when the conversation is finished, I will share the final report.\n",
        "Rate the agent out of 100 on the output, then calculate the expected score for a real medical professional on the same patient info,\n",
        "and compare their performance briefly.\n",
        "\"\"\"\n",
        "# Test case with expected output to be heart attack, if AI asks additional questions, use chatGPT to answer those while keeping the expected diagnosis same.\n",
        "sample_input = \"Hi, I've been feeling really off lately. \" \\\n",
        "\"For the past few hours, Iâ€™ve had some chest discomfort, \" \\\n",
        "\"but itâ€™s not exactly pain. Itâ€™s more of a pressure, kind of like \" \\\n",
        "\"something heavy is on my chest. I also feel really short of breath, \" \\\n",
        "\"especially when I try to move around or even just stand up. Sometimes, it \" \\\n",
        "\"feels like my left arm is a little sore, and I've noticed some dizziness as well. \" \\\n",
        "\"Iâ€™m also feeling unusually nauseous, which isnâ€™t something I usually deal with. \" \\\n",
        "\"Iâ€™m 45, not very active, and have had some family members with heart issues. \" \\\n",
        "\"Iâ€™m not sure if this is something I should be concerned about or if Iâ€™m just \" \\\n",
        "\"overthinking it. Can you help?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BvP_8bKHgPz2",
        "outputId": "eb419e1c-2362-4d33-f3d1-1926e0dcabfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Medical Symptom Analysis Chatbot ---\n",
            "Describe your symptoms to start.\n",
            "Type 'exit' to end.\n",
            "Type 'new topic' (or similar) after analysis to discuss something else.\n",
            "\n",
            "START: Invoking graph (initial)...\n",
            "PROCESSING: Entering interactive_conversation node...\n",
            "PROCESSING: (interactive_conversation) Extracting details from latest user message...\n",
            "DEBUG: Extracting symptom details...\n",
            "DEBUG: Symptom extraction complete.\n",
            "DEBUG: Invoking LLM for conversation (Turn 1, assessing sufficiency, expecting JSON)...\n",
            "DEBUG: LLM raw response received: '```json\n",
            "{\n",
            "  \"proceed_to_research\": false,\n",
            "  \"assistant_message\": \"To better understand your IT band ...'\n",
            "DEBUG: JSON parsed successfully. proceed_to_research=False\n",
            "DEBUG: Based on parsed JSON/error handling: enough info? False. New stage: conversation\n",
            "THINKING: Determining next stage... Current stage: conversation\n",
            "PROCESSING: Routing to END (yielding for user input).\n",
            "UPDATE: Graph invocation complete (initial).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #00afff; text-decoration-color: #00afff; font-weight: bold\">[Assistant]:</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;38;5;39m[\u001b[0m\u001b[1;38;5;39mAssistant\u001b[0m\u001b[1;38;5;39m]\u001b[0m\u001b[1;38;5;39m:\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">To better understand your IT band pain, could you please describe where exactly you feel the pain?                 \n",
              "</pre>\n"
            ],
            "text/plain": [
              "To better understand your IT band pain, could you please describe where exactly you feel the pain?                 \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PROCESSING: Invoking graph (continue)...\n",
            "PROCESSING: Entering interactive_conversation node...\n",
            "PROCESSING: (interactive_conversation) Extracting details from latest user message...\n",
            "DEBUG: Extracting symptom details...\n",
            "DEBUG: Symptom extraction complete.\n",
            "DEBUG: Invoking LLM for conversation (Turn 2, assessing sufficiency, expecting JSON)...\n",
            "DEBUG: LLM raw response received: '```json\n",
            "{\n",
            "  \"proceed_to_research\": false,\n",
            "  \"assistant_message\": \"Can you describe the character of ...'\n",
            "DEBUG: JSON parsed successfully. proceed_to_research=False\n",
            "DEBUG: Based on parsed JSON/error handling: enough info? False. New stage: conversation\n",
            "THINKING: Determining next stage... Current stage: conversation\n",
            "PROCESSING: Routing to END (yielding for user input).\n",
            "UPDATE: Graph invocation complete (continue).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #00afff; text-decoration-color: #00afff; font-weight: bold\">[Assistant]:</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;38;5;39m[\u001b[0m\u001b[1;38;5;39mAssistant\u001b[0m\u001b[1;38;5;39m]\u001b[0m\u001b[1;38;5;39m:\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Can you describe the character of the pain in your thigh? Is it sharp, dull, aching, or burning?                   \n",
              "</pre>\n"
            ],
            "text/plain": [
              "Can you describe the character of the pain in your thigh? Is it sharp, dull, aching, or burning?                   \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PROCESSING: Invoking graph (continue)...\n",
            "PROCESSING: Entering interactive_conversation node...\n",
            "PROCESSING: (interactive_conversation) Extracting details from latest user message...\n",
            "DEBUG: Extracting symptom details...\n",
            "DEBUG: Symptom extraction complete.\n",
            "DEBUG: Invoking LLM for conversation (Turn 3, assessing sufficiency, expecting JSON)...\n",
            "DEBUG: LLM raw response received: '```json\n",
            "{\n",
            "  \"proceed_to_research\": false,\n",
            "  \"assistant_message\": \"To understand the potential cause ...'\n",
            "DEBUG: JSON parsed successfully. proceed_to_research=False\n",
            "DEBUG: Based on parsed JSON/error handling: enough info? False. New stage: conversation\n",
            "THINKING: Determining next stage... Current stage: conversation\n",
            "PROCESSING: Routing to END (yielding for user input).\n",
            "UPDATE: Graph invocation complete (continue).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #00afff; text-decoration-color: #00afff; font-weight: bold\">[Assistant]:</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;38;5;39m[\u001b[0m\u001b[1;38;5;39mAssistant\u001b[0m\u001b[1;38;5;39m]\u001b[0m\u001b[1;38;5;39m:\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">To understand the potential cause of the ache, can you tell me when the pain in your thigh started?                \n",
              "</pre>\n"
            ],
            "text/plain": [
              "To understand the potential cause of the ache, can you tell me when the pain in your thigh started?                \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "start_interactive_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyVCROTzEzBd"
      },
      "source": [
        "# Workshop Recap\n",
        "\n",
        "Congratulations! You've built and interacted with a multi-step AI agent using LangGraph.\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "* **LLMs can orchestrate:** They don't just generate text; they can follow steps, use tools, and make autonomous decisions within a defined structure.\n",
        "* **LangGraph provides structure:** It allows us to build complex, stateful AI workflows reliably by defining nodes (steps) and edges (transitions).\n",
        "* **State is crucial:** Managing the conversation history, intermediate results, and current stage is essential for multi-turn interactions for complex tasks.\n",
        "* **Tools enhance LLMs:** Giving LLMs access to external APIs or functions dramatically increases their capabilities, making them experts at tasks.\n",
        "* **Prompting is key:** Carefully crafted prompts (System prompts, prompts for nodes. tools) guide the AI's behavior.\n",
        "\n",
        "**Further Exploration:**\n",
        "\n",
        "* Add more tools (e.g., a calculator, a database lookup).\n",
        "* Experiment with different LLMs or prompt strategies on new use cases.\n",
        "* Explore LangSmith for debugging and tracing your graph runs."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}